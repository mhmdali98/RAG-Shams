"""
rebuild_vectorstore.py
Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© Ø¨Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø­Ø³Ù‘Ù†Ø© + Ø¯Ø¹Ù… metadata
"""

from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
import shutil
import os
import re

print("âš™ï¸ Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© Ø¨Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø­Ø³Ù‘Ù†Ø©...")

# Ø­Ø°Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©
if os.path.exists("./chroma_db"):
    print("ğŸ—‘ï¸  Ø­Ø°Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©...")
    shutil.rmtree("./chroma_db")

# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Øµ
print("ğŸ“„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Øµ...")
loader = TextLoader("shams-info.txt", encoding="utf-8")
documents = loader.load()
print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(documents)} Ù…Ø³ØªÙ†Ø¯")

# 2. Ø¥Ø«Ø±Ø§Ø¡ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø¨Ù€ metadata Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
print("ğŸ·ï¸  Ø¬Ø§Ø±ÙŠ Ø¥Ø«Ø±Ø§Ø¡ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø¨Ù€ metadata...")
full_text = documents[0].page_content

# ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ù‚Ø³Ø§Ù… Ø±Ø¦ÙŠØ³ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
sections = re.split(r"(?=\n===\s.+?\s===)", full_text)
enhanced_docs = []

for section in sections:
    if not section.strip():
        continue

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù‚Ø³Ù…
    header_match = re.search(r"===\s*(.+?)\s*===", section)
    section_name = header_match.group(1).strip() if header_match else "Ø¹Ø§Ù…"

    # ØªØµÙÙŠØ© Ù…Ø­ØªÙˆÙ‰ ØºÙŠØ± Ù…ÙÙŠØ¯ Ù„Ù€ RAG (Ù…Ø«Ù„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©)
    if "Ø£Ø®Ø¨Ø§Ø± ÙˆÙ…Ù‚Ø§Ù„Ø§Øª" in section_name or "Ø´Ø§Ø±ÙƒÙ†Ø§ ÙÙŠ ÙØ¹Ø§Ù„ÙŠØ©" in section:
        # Ù†Ø¨Ù‚ÙŠ ÙÙ‚Ø· Ø³Ø·Ø±ÙŠÙ† Ù…ÙˆØ¬Ø²ÙŠÙ† Ù…Ù† Ø§Ù„Ø£Ø®Ø¨Ø§Ø± (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        lines = section.split("\n")
        brief_section = "\n".join([lines[0], *[line for line in lines[1:4] if line.strip()]])
        content = brief_section
    else:
        content = section

    enhanced_docs.append({
        "page_content": content.strip(),
        "metadata": {"section": section_name}
    })

print(f"âœ… ØªÙ… ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ {len(enhanced_docs)} Ù‚Ø³Ù…")

# 3. ØªØ¬Ø²Ø¦Ø© Ø°ÙƒÙŠØ© Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ metadata
print("âœ‚ï¸  Ø¬Ø§Ø±ÙŠ ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ù†Øµ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø­Ø³Ù‘Ù†Ø©...")
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=80,
    separators=["\n\n=== ", "\n\n---\n\n", "\n\n", "\n", ". ", "ØŒ ", " ", ""],
    length_function=len,
)

final_chunks = []
for doc in enhanced_docs:
    chunks = text_splitter.split_text(doc["page_content"])
    for chunk in chunks:
        if chunk.strip():  # ØªØ¬Ù†Ø¨ Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„ÙØ§Ø±ØºØ©
            final_chunks.append({
                "page_content": chunk.strip(),
                "metadata": doc["metadata"]
            })

print(f"âœ… ØªÙ… ØªÙˆÙ„ÙŠØ¯ {len(final_chunks)} Ø¬Ø²Ø¡Ù‹Ø§ Ø¬Ø§Ù‡Ø²Ù‹Ø§ Ù„Ù„ØªØ¶Ù…ÙŠÙ†")

# 4. ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†
print("ğŸ”¢ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†...")
embeddings = HuggingFaceEmbeddings(
    model_name="all-MiniLM-L6-v2",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)

test_vec = embeddings.embed_query("ØªØ¬Ø±Ø¨Ø©")
print(f"âœ… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø¬Ø§Ù‡Ø²! Ø·ÙˆÙ„ Ø§Ù„Ù…ØªØ¬Ù‡: {len(test_vec)}")

# 5. Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø³ØªÙ†Ø¯Ø§Øª LangChain Ù…Ø¹ metadata
from langchain_core.documents import Document

langchain_docs = [
    Document(page_content=item["page_content"], metadata=item["metadata"])
    for item in final_chunks
]

# 6. Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ¬Ù‡Ø© Ø¬Ø¯ÙŠØ¯Ø©
print("ğŸ’¾ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©...")
vectorstore = Chroma.from_documents(
    documents=langchain_docs,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

vectorstore.persist()
print("âœ… ØªÙ… Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© ÙÙŠ './chroma_db'")

# 7. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù…Ø¹ Ø¹Ø±Ø¶ metadata
print("\nğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª...")
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"k": 4, "score_threshold": 0.25}
)

test_queries = [
    "Ù…Ø§ Ø³Ø¹Ø± Ø¨Ø§Ù‚Ø© ÙØ§ÙŠØ¨Ø± 75ØŸ",
    "Ù‡Ù„ Ù„Ø¯ÙŠÙƒÙ… Ø¯Ø¹Ù… ÙÙ†ÙŠ 24 Ø³Ø§Ø¹Ø©ØŸ",
    "Ù…Ø§ Ù‡ÙŠ Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„ØªØºØ·ÙŠØ©ØŸ",
    "Ù…Ù† Ù‡Ù… Ø´Ø±ÙƒØ§Ø¤ÙƒÙ…ØŸ"
]

for query in test_queries:
    print(f"\nâ“ Ø§Ù„Ø³Ø¤Ø§Ù„: {query}")
    results = retriever.invoke(query)
    print(f"   ğŸ“Š ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ {len(results)} Ù†ØªÙŠØ¬Ø©")
    if results:
        print(f"   ğŸ“‚ Ø§Ù„Ù‚Ø³Ù…: {results[0].metadata.get('section', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}")
        print(f"   ğŸ“„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {results[0].page_content[:120]}...")

print("\nâœ… Ø§ÙƒØªÙ…Ù„Øª Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© Ø¨Ù†Ø¬Ø§Ø­!")
print("ğŸ’¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø¶Ø§ÙØ©:")
print("   - Ø¯Ø¹Ù… metadata Ù„ÙƒÙ„ Ø¬Ø²Ø¡")
print("   - ØªØµÙÙŠØ© Ø°ÙƒÙŠØ© Ù„Ù„Ù…Ø­ØªÙˆÙ‰ ØºÙŠØ± Ø§Ù„Ù…Ø¨Ø§Ø´Ø± (Ù…Ø«Ù„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±)")
print("   - Ø§Ø³ØªØ®Ø¯Ø§Ù… score_threshold Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø©")
print("   - ØªØ¬Ø²Ø¦Ø© Ù…Ø±Ù†Ø© ØªØ­ØªØ±Ù… Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£ØµÙ„ÙŠ")