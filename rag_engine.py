"""
rag_engine.py
Ù…Ø­Ø±Ùƒ RAG Ù…Ø­Ø³Ù‘Ù† Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© ÙˆØ§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡
Ø­ÙˆÙ„ Ø´Ø±ÙƒØ© Ø§Ù„Ø´Ù…Ø³ ØªÙŠÙ„ÙŠÙƒÙˆÙ….
"""

import logging
import re
from typing import List, Optional, Tuple
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# === ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… ===
try:
    logger.info("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†...")
    embeddings = HuggingFaceEmbeddings(
        model_name="all-MiniLM-L6-v2",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True}
    )
    
    logger.info("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©...")
    vectorstore = Chroma(persist_directory="./chroma_db", embedding_function=embeddings)
    
    # Retriever Ø°ÙƒÙŠ: ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† k Ø¹Ø§Ù„ÙŠ ÙˆØ¹ØªØ¨Ø© ØªØ´Ø§Ø¨Ù‡ Ù…Ø­Ø³Ù‘Ù†Ø©
    retriever = vectorstore.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={"k": 10, "score_threshold": 0.2}  # Ø²ÙŠØ§Ø¯Ø© k ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¹ØªØ¨Ø©
    )
    
    # Retriever Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„ÙˆØ§Ø³Ø¹
    fallback_retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 5}
    )
    
    logger.info("Ø¬Ø§Ø±ÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ...")
    llm = ChatOllama(model="llama3", temperature=0.01, num_ctx=4096)  # Ø£Ù‚Ù„ Ø­Ø±Ø§Ø±Ø© = Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©
    
    logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ù†Ø¬Ø§Ø­")
    
except Exception as e:
    logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù…: {str(e)}")
    raise

# ØªØµØ¯ÙŠØ± vectorstore Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ modules Ø£Ø®Ø±Ù‰
__all__ = ['get_answer', 'vectorstore', 'retriever', 'llm']

# Ù‚Ø§Ù„Ø¨ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…Ø­Ø³Ù‘Ù† ÙˆÙ…Ø­ØªØ±Ù
prompt = ChatPromptTemplate.from_messages([
    ("system", """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ø§Ø­ØªØ±Ø§ÙÙŠ Ù„Ø´Ø±ÙƒØ© "Ø§Ù„Ø´Ù…Ø³ ØªÙŠÙ„ÙŠÙƒÙˆÙ…". Ù‚ÙˆØ§Ø¹Ø¯Ùƒ Ø§Ù„ØµØ§Ø±Ù…Ø©:

**Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:**
1. **Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·** - Ù…Ù…Ù†ÙˆØ¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙŠ Ù„ØºØ© Ø£Ø®Ø±Ù‰.
2. **Ø§Ø³ØªØ®Ù„Øµ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚** - Ù…Ù…Ù†ÙˆØ¹ Ø§Ù„Ø§Ø®ØªÙ„Ø§Ù‚ Ø£Ùˆ Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ù…Ù† Ø®Ø§Ø±Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚.
3. **Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚**ØŒ Ù‚Ù„ Ø¨ÙˆØ¶ÙˆØ­: "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§ØªÙ†Ø§ Ø§Ù„Ø­Ø§Ù„ÙŠØ©."

**Ø¹Ù†Ø¯ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø¨Ø§Ù‚Ø§Øª Ø£Ùˆ Ø§Ù„Ø£Ø³Ø¹Ø§Ø±:**
- Ø§Ø°ÙƒØ± **Ø¬Ù…ÙŠØ¹** Ø§Ù„Ø¨Ø§Ù‚Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚
- Ù„ÙƒÙ„ Ø¨Ø§Ù‚Ø©: Ø§Ù„Ø§Ø³Ù…ØŒ Ø§Ù„Ø³Ø¹Ø± (Ø¨Ø§Ù„Ø¯ÙŠÙ†Ø§Ø± Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠ)ØŒ Ø§Ù„Ù†ÙˆØ¹ (ÙØ§ÙŠØ¨Ø±/ÙˆØ§ÙŠØ±Ù„Ø³/Ù…Ù†ØµØ©)ØŒ Ø§Ù„Ø³Ø±Ø¹Ø© (Ø¥Ù† ÙˆØ¬Ø¯Øª)ØŒ ÙˆØµÙ Ù…ÙˆØ¬Ø²
- Ù†Ø¸Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù‚Ø§Ø· Ø£Ùˆ Ø¬Ø¯Ø§ÙˆÙ„

**Ø¹Ù†Ø¯ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø®Ø¯Ù…Ø§Øª:**
- Ø§Ø°ÙƒØ± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚
- Ø§Ø°ÙƒØ± Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ù‡Ù…Ø© (Ù…Ø«Ù„: 70,000 Ø®Ø· ÙƒØ§Ø¨Ù„ Ø¶ÙˆØ¦ÙŠ)

**Ø¹Ù†Ø¯ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„ØªÙˆØ§ØµÙ„:**
- Ø§Ø°ÙƒØ±: Ø§Ù„Ù‡Ø§ØªÙ (6449)ØŒ Ø§Ù„Ø¨Ø±ÙŠØ¯ (info@shams-tele.com)ØŒ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ØŒ ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ

**Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:**
- Ø§Ø­ØªØ±Ø§ÙÙŠØŒ ÙˆØ¯ÙˆØ¯ØŒ ÙˆÙˆØ§Ø¶Ø­
- Ø§Ø³ØªØ®Ø¯Ù… Ø¹Ù†Ø§ÙˆÙŠÙ† ÙØ±Ø¹ÙŠØ© Ø£Ùˆ Ù†Ù‚Ø§Ø· Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
- Ù„Ø§ ØªÙƒØ±Ø± Ù†ÙØ³ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø©
- ÙƒÙ† Ù…ÙˆØ¬Ø²Ù‹Ø§ Ù„ÙƒÙ† Ø´Ø§Ù…Ù„Ù‹Ø§

**Ù…Ù…Ù†ÙˆØ¹ ØªÙ…Ø§Ù…Ù‹Ø§:**
- Ø§Ù„Ø§Ø®ØªÙ„Ø§Ù‚ Ø£Ùˆ Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ù…Ù† Ø®Ø§Ø±Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚
- Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„ØºØ§Øª ØºÙŠØ± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
- Ø¥Ø¹Ø·Ø§Ø¡ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚
- Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ù…ÙØ±Ø·

**ØªØ£ÙƒØ¯ Ù…Ù†:**
- Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·
- Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙÙ‚Ø·
- Ø§Ù„Ø¯Ù‚Ø© ÙÙŠ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ø£Ø³Ù…Ø§Ø¡
"""),
    ("human", """**Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…ØªØ§Ø­ (Ø§Ø³ØªØ®Ø¯Ù…Ù‡ ÙÙ‚Ø· ÙƒÙ…ØµØ¯Ø± Ù„Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª):**
{context}

**Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¹Ù…ÙŠÙ„:**
{input}

**Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ Ø£Ø¹Ù„Ø§Ù‡ Ø­ØµØ±ÙŠÙ‹Ø§:**""")
])


def filter_and_deduplicate_docs(docs: List[Document]) -> str:
    """
    ØªØµÙÙŠØ© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ÙÙŠØ¯Ø© + Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± + Ø§Ù„Ø­Ø¯ Ù…Ù† Ø§Ù„Ø·ÙˆÙ„
    """
    if not docs:
        return "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ©."
    
    # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø·ÙˆÙŠÙ„Ø© (ØºÙŠØ± Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…)
    filtered = []
    for doc in docs:
        content = doc.page_content.strip()
        if not content:
            continue
            
        # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…Ø«Ù„ "Ø´Ø§Ø±ÙƒÙ†Ø§ ÙÙŠ ÙØ¹Ø§Ù„ÙŠØ©" Ø£Ùˆ "ÙˆØ±Ø´Ø© Ø¹Ù…Ù„"
        if any(term in content for term in ["Ø´Ø§Ø±ÙƒÙ†Ø§ ÙÙŠ ÙØ¹Ø§Ù„ÙŠØ©", "ÙˆØ±Ø´Ø© Ø¹Ù…Ù„", "Ø±Ø¹Ø§Ø©", "Ø­Ø¯Ø«", "Ù…Ø¤ØªÙ…Ø±"]):
            if len(content) > 300:
                # Ù†Ø¨Ù‚ÙŠ ÙÙ‚Ø· Ø¬Ù…Ù„Ø© Ù…ÙˆØ¬Ø²Ø© Ø¥Ù† ÙˆÙØ¬Ø¯Øª
                first_line = content.split("\n")[0]
                if any(keyword in first_line for keyword in ["Ø´Ù…Ø³", "Ø¨Ø§Ù‚Ø©", "Ø³Ø¹Ø±", "Ø¯ÙŠÙ†Ø§Ø±", "FTTH", "WiFi"]):
                    filtered.append(first_line)
                continue
        filtered.append(content)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± ØªÙ‚Ø±ÙŠØ¨ÙŠÙ‹Ø§ (Ù…Ø­Ø³Ù‘Ù†)
    unique_contents = []
    seen = set()
    for text in filtered:
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… hash Ø£ÙØ¶Ù„ Ù„Ù„ØªÙƒØ±Ø§Ø±
        text_lower = text.lower().strip()
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØªØ§Ø­ ÙØ±ÙŠØ¯ Ù…Ù† Ø£ÙˆÙ„ 100 Ø­Ø±Ù
        key = text_lower[:100] if len(text_lower) > 100 else text_lower
        if key not in seen:
            seen.add(key)
            unique_contents.append(text)
    
    # Ø¯Ù…Ø¬ Ø§Ù„Ù†Øµ Ù…Ø¹ Ø­Ø¯ Ø£Ù‚ØµÙ‰ ~2000 Ø­Ø±Ù (Ø²ÙŠØ§Ø¯Ø© Ù„Ù„Ø¯Ù‚Ø©)
    context = "\n---\n".join(unique_contents)
    return context[:2000]


# Ø³Ù„Ø³Ù„Ø© RAG Ù…Ø­Ø³Ù‘Ù†Ø©
rag_chain = (
    {"context": retriever | filter_and_deduplicate_docs, "input": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)


def is_arabic_text(text: str) -> bool:
    """ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ Ø¹Ø±Ø¨ÙŠÙ‹Ø§ Ø¨Ø´ÙƒÙ„ Ø£Ø³Ø§Ø³ÙŠ"""
    if not text or not text.strip():
        return False
    
    # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    arabic_chars = sum(1 for char in text if '\u0600' <= char <= '\u06FF' or char in 'ØŒØ›ØŸ')
    total_chars = len([c for c in text if c.isalpha() or c in 'ØŒØ›ØŸ'])
    
    if total_chars == 0:
        return False
    
    arabic_ratio = arabic_chars / total_chars if total_chars > 0 else 0
    return arabic_ratio >= 0.3  # Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ 30% Ø¹Ø±Ø¨ÙŠ


def validate_answer(answer: str, context: str) -> Tuple[bool, str]:
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©"""
    if not answer or len(answer.strip()) < 10:
        return False, "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ù‹Ø§"
    
    # ÙØ­Øµ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    if not is_arabic_text(answer):
        return False, "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù„ÙŠØ³Øª Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
    
    # ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙÙŠØ¯Ø©
    if any(phrase in answer.lower() for phrase in ["Ù„Ø§ Ø£Ø¹Ø±Ù", "Ù„Ø§ Ø£Ù…Ù„Ùƒ", "Ù„Ø§ ÙŠÙˆØ¬Ø¯", "ØºÙŠØ± Ù…ØªÙˆÙØ±"]):
        if "Ø¹Ø°Ø±Ù‹Ø§" not in answer and "Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§ØªÙ†Ø§" not in answer:
            return False, "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØºÙŠØ± ÙˆØ§Ø¶Ø­Ø©"
    
    # ÙØ­Øµ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ù…ÙØ±Ø·
    words = answer.split()
    if len(set(words)) < len(words) * 0.3:  # Ø£ÙƒØ«Ø± Ù…Ù† 70% ØªÙƒØ±Ø§Ø±
        return False, "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªÙƒØ±Ø§Ø± Ù…ÙØ±Ø·"
    
    return True, "ØµØ­ÙŠØ­Ø©"


def expand_query(question: str) -> str:
    """ØªÙˆØ³ÙŠØ¹ Ø°ÙƒÙŠ Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø£Ù†Ù…Ø§Ø· Ø´Ù…Ø³ ØªÙŠÙ„ÙŠÙƒÙˆÙ…"""
    q = question.strip().lower()
    original = question.strip()
    
    # Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© (Ù…Ø­Ø³Ù‘Ù†Ø©)
    if any(w in q for w in ["Ø§Ù„Ø³Ø¹Ø±", "Ø³Ø¹Ø±", "ÙƒÙ…", "Ø¯ÙŠÙ†Ø§Ø±", "ØªÙƒÙ„ÙØ©", "Ø«Ù…Ù†"]):
        return f"{original} Ø¨Ø§Ù‚Ø§Øª Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ø§Ù„Ø£Ø³Ø¹Ø§Ø± FTTH WiFi Ø¯ÙŠÙ†Ø§Ø± Ø¹Ø±Ø§Ù‚ÙŠ ÙØ§ÙŠØ¨Ø± ÙˆØ§ÙŠØ±Ù„Ø³"
    
    if any(w in q for w in ["Ø¨Ø§Ù‚Ø©", "Ø§Ø´ØªØ±Ø§Ùƒ", "Ø§Ù„Ø¨Ø§Ù‚Ø§Øª", "Ø¨Ø§Ù‚Ø§Øª", "Ø®Ø·Ø©"]):
        return f"{original} Ø¨Ø§Ù‚Ø§Øª Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª FTTH WiFi Ø§Ù„Ù…Ù†ØµØ© ÙØ§ÙŠØ¨Ø± Star Sun Neptune Galaxy Star"
    
    if any(w in q for w in ["ØªØºØ·ÙŠØ©", "Ù…Ù†Ø·Ù‚Ø©", "Ø£ÙŠÙ†", "ÙØ±Ø¹", "Ù…ÙƒØ§Ù†"]):
        return f"{original} ØªØºØ·ÙŠØ© Ø¨ØºØ¯Ø§Ø¯ Ø¯ÙŠØ§Ù„Ù‰ Ø¨Ø§Ø¨Ù„ Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø§Øª ÙØ±Ø¹ Ù…ÙˆÙ‚Ø¹"
    
    if any(w in q for w in ["Ø¯Ø¹Ù…", "Ù…Ø³Ø§Ø¹Ø¯Ù‡", "24", "Ø®Ø¯Ù…Ø©", "Ø§ØªØµØ§Ù„"]):
        return f"{original} Ø¯Ø¹Ù… ÙÙ†ÙŠ 24/7 Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ ØªÙˆØ§ØµÙ„"
    
    if any(w in q for w in ["Ø´Ø±ÙƒØ©", "Ù…Ù† Ù†Ø­Ù†", "Ø¹Ù†", "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"]):
        return f"{original} Ø´Ø±ÙƒØ© Ø´Ù…Ø³ ØªÙŠÙ„ÙŠÙƒÙˆÙ… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„Ø´Ø±ÙƒØ©"
    
    return original


def get_answer(question: str, max_retries: int = 2) -> str:
    """
    Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø© ÙˆØ³Ø±ÙŠØ¹Ø© Ù…Ø¹ Ù†Ø¸Ø§Ù… Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© ÙˆØ§Ù„ØªØ­Ù‚Ù‚
    """
    if not question or not question.strip():
        return "Ù…Ø±Ø­Ø¨Ø§Ù‹! ğŸŒ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ Ø§Ø³Ø£Ù„Ù†ÙŠ Ø¹Ù† Ø¨Ø§Ù‚Ø§Øª Ø§Ù„Ø¥Ù†ØªØ±Ù†ØªØŒ Ø§Ù„Ø£Ø³Ø¹Ø§Ø±ØŒ Ø§Ù„ØªØºØ·ÙŠØ©ØŒ Ø£Ùˆ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø´Ù…Ø³ ØªÙŠÙ„ÙŠÙƒÙˆÙ…."

    clean_q = question.strip()
    logger.info(f"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: '{clean_q}'")

    # ÙØ­Øµ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„Ø³Ø¤Ø§Ù„
    if not is_arabic_text(clean_q):
        logger.warning(f"Ø§Ù„Ø³Ø¤Ø§Ù„ Ù‚Ø¯ Ù„Ø§ ÙŠÙƒÙˆÙ† Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: '{clean_q}'")

    # ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
    expanded = expand_query(clean_q)
    if expanded != clean_q:
        logger.debug(f"Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…ÙˆØ³Ù‘Ø¹: {expanded}")

    context_used = ""
    
    try:
        # Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø£ÙˆÙ„ÙŠ
        docs = retriever.invoke(expanded)
        logger.info(f"ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ {len(docs)} Ù…Ø³ØªÙ†Ø¯(Ø§Øª)")

        # Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ØŒ Ù†Ø­Ø§ÙˆÙ„ Ø¨Ø®Ø·ÙˆØ§Øª Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
        if not docs:
            fallback_queries = [
                clean_q,
                " ".join([w for w in clean_q.split() if len(w) > 2]),
                "Ø¨Ø§Ù‚Ø§Øª Ø£Ø³Ø¹Ø§Ø± Ø¯Ø¹Ù… ØªÙˆØ§ØµÙ„"
            ]
            for fq in fallback_queries:
                docs = retriever.invoke(fq)
                if docs:
                    logger.info(f"ØªÙ… Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ: {fq}")
                    break
            
            # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø´ÙŠØ¦Ù‹Ø§ØŒ Ù†Ø³ØªØ®Ø¯Ù… retriever Ø§Ø­ØªÙŠØ§Ø·ÙŠ
            if not docs:
                docs = fallback_retriever.invoke(clean_q)
                logger.info(f"ØªÙ… Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… retriever Ø§Ø­ØªÙŠØ§Ø·ÙŠ: {len(docs)} Ù…Ø³ØªÙ†Ø¯(Ø§Øª)")

        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø³ÙŠØ§Ù‚
        context_used = filter_and_deduplicate_docs(docs)
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©
        response = None
        for attempt in range(max_retries + 1):
            try:
                response = rag_chain.invoke(clean_q)
                response = response.strip()
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
                is_valid, validation_msg = validate_answer(response, context_used)
                if is_valid:
                    logger.info(f"ØªÙ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1})")
                    break
                else:
                    logger.warning(f"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØºÙŠØ± ØµØ­ÙŠØ­Ø©: {validation_msg} (Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1})")
                    if attempt < max_retries:
                        # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø¹ prompt Ù…Ø­Ø³Ù‘Ù†
                        continue
                    else:
                        # Ø¥Ø°Ø§ ÙØ´Ù„Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§ØªØŒ Ù†Ø¹ÙŠØ¯ ØµÙŠØ§ØºØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
                        response = "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§ØªÙ†Ø§ Ø§Ù„Ø­Ø§Ù„ÙŠØ©.\n\nÙ‡Ù„ ØªØ±ÙŠØ¯ Ù…Ø¹Ø±ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø¨Ø§Ù‚Ø§ØªÙ†Ø§ Ø£Ùˆ Ø®Ø¯Ù…Ø§ØªÙ†Ø§ØŸ ğŸ˜Š"
            except Exception as e:
                logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}: {e}")
                if attempt < max_retries:
                    continue
                else:
                    raise
        
        # ØªÙ†Ø¸ÙŠÙ Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø©
        if not response:
            response = "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§ØªÙ†Ø§ Ø§Ù„Ø­Ø§Ù„ÙŠØ©.\n\nÙ‡Ù„ ØªØ±ÙŠØ¯ Ù…Ø¹Ø±ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø¨Ø§Ù‚Ø§ØªÙ†Ø§ Ø£Ùˆ Ø®Ø¯Ù…Ø§ØªÙ†Ø§ØŸ ğŸ˜Š"
        
        # Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ Ù†Øµ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ ØºÙŠØ± Ø¶Ø±ÙˆØ±ÙŠ (Ù…Ø«Ù„ "Answer:" Ø£Ùˆ "Response:")
        response = re.sub(r'^(Answer|Response|Reply):\s*', '', response, flags=re.IGNORECASE)
        response = response.strip()
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        if not is_arabic_text(response):
            logger.warning("Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù‚Ø¯ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Øµ ØºÙŠØ± Ø¹Ø±Ø¨ÙŠØŒ Ø³ÙŠØªÙ… Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©...")
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø£Ø®ÙŠØ±Ø©
            try:
                response = rag_chain.invoke(f"{clean_q}\n\nØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·.")
                response = response.strip()
            except:
                pass
        
        return response

    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {e}", exc_info=True)
        return "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ù‹Ø§ Ø£Ùˆ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù†Ø§ Ø¹Ù„Ù‰ 6449."