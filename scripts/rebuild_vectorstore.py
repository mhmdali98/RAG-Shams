"""
rebuild_vectorstore.py
Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©
"""

import sys
from pathlib import Path

BASE_DIR = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(BASE_DIR))

from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
import shutil
import os
import re

from config import Settings

print("âš™ï¸ Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© Ø¨Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø­Ø³Ù‘Ù†Ø©...")

# Ø­Ø°Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
if Settings.CHROMA_DB_DIR.exists():
    print("ğŸ—‘ï¸  Ø­Ø°Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©...")
    shutil.rmtree(Settings.CHROMA_DB_DIR)

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
Settings.CHROMA_DB_DIR.mkdir(parents=True, exist_ok=True)

# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Øµ
print("ğŸ“„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Øµ...")
loader = TextLoader(str(Settings.DATA_FILE), encoding="utf-8")
documents = loader.load()
print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(documents)} Ù…Ø³ØªÙ†Ø¯")

# 2. Ø¥Ø«Ø±Ø§Ø¡ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø¨Ù€ metadata
print("ğŸ·ï¸  Ø¬Ø§Ø±ÙŠ Ø¥Ø«Ø±Ø§Ø¡ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø¨Ù€ metadata...")
full_text = documents[0].page_content

sections = re.split(r"(?=\n===\s.+?\s===)", full_text)
enhanced_docs = []

for section in sections:
    if not section.strip():
        continue

    header_match = re.search(r"===\s*(.+?)\s*===", section)
    section_name = header_match.group(1).strip() if header_match else "Ø¹Ø§Ù…"

    if "Ø£Ø®Ø¨Ø§Ø± ÙˆÙ…Ù‚Ø§Ù„Ø§Øª" in section_name or "Ø´Ø§Ø±ÙƒÙ†Ø§ ÙÙŠ ÙØ¹Ø§Ù„ÙŠØ©" in section:
        lines = section.split("\n")
        brief_section = "\n".join([lines[0], *[line for line in lines[1:4] if line.strip()]])
        content = brief_section
    else:
        content = section

    enhanced_docs.append({
        "page_content": content.strip(),
        "metadata": {"section": section_name}
    })

print(f"âœ… ØªÙ… ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ {len(enhanced_docs)} Ù‚Ø³Ù…")

# 3. ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ù†Øµ
print("âœ‚ï¸  Ø¬Ø§Ø±ÙŠ ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ù†Øµ...")
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=80,
    separators=["\n\n=== ", "\n\n---\n\n", "\n\n", "\n", ". ", "ØŒ ", " ", ""],
    length_function=len,
)

final_chunks = []
for doc in enhanced_docs:
    chunks = text_splitter.split_text(doc["page_content"])
    for chunk in chunks:
        if chunk.strip():
            final_chunks.append({
                "page_content": chunk.strip(),
                "metadata": doc["metadata"]
            })

print(f"âœ… ØªÙ… ØªÙˆÙ„ÙŠØ¯ {len(final_chunks)} Ø¬Ø²Ø¡Ù‹Ø§ Ø¬Ø§Ù‡Ø²Ù‹Ø§ Ù„Ù„ØªØ¶Ù…ÙŠÙ†")

# 4. ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†
print("ğŸ”¢ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†...")
embeddings = HuggingFaceEmbeddings(
    model_name=Settings.EMBEDDING_MODEL,
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)

test_vec = embeddings.embed_query("ØªØ¬Ø±Ø¨Ø©")
print(f"âœ… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø¬Ø§Ù‡Ø²! Ø·ÙˆÙ„ Ø§Ù„Ù…ØªØ¬Ù‡: {len(test_vec)}")

# 5. Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø³ØªÙ†Ø¯Ø§Øª LangChain
langchain_docs = [
    Document(page_content=item["page_content"], metadata=item["metadata"])
    for item in final_chunks
]

# 6. Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ¬Ù‡Ø©
print("ğŸ’¾ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©...")
vectorstore = Chroma.from_documents(
    documents=langchain_docs,
    embedding=embeddings,
    persist_directory=str(Settings.CHROMA_DB_DIR)
)

vectorstore.persist()
print(f"âœ… ØªÙ… Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© ÙÙŠ '{Settings.CHROMA_DB_DIR}'")

# 7. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹
print("\nğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª...")
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"k": 4, "score_threshold": 0.25}
)

test_queries = [
    "Ù…Ø§ Ø³Ø¹Ø± Ø¨Ø§Ù‚Ø© ÙØ§ÙŠØ¨Ø± 75ØŸ",
    "Ù‡Ù„ Ù„Ø¯ÙŠÙƒÙ… Ø¯Ø¹Ù… ÙÙ†ÙŠ 24 Ø³Ø§Ø¹Ø©ØŸ",
    "Ù…Ø§ Ù‡ÙŠ Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„ØªØºØ·ÙŠØ©ØŸ",
    "Ù…Ù† Ù‡Ù… Ø´Ø±ÙƒØ§Ø¤ÙƒÙ…ØŸ"
]

for query in test_queries:
    print(f"\nâ“ Ø§Ù„Ø³Ø¤Ø§Ù„: {query}")
    results = retriever.invoke(query)
    print(f"   ğŸ“Š ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ {len(results)} Ù†ØªÙŠØ¬Ø©")
    if results:
        print(f"   ğŸ“‚ Ø§Ù„Ù‚Ø³Ù…: {results[0].metadata.get('section', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}")
        print(f"   ğŸ“„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {results[0].page_content[:120]}...")

print("\nâœ… Ø§ÙƒØªÙ…Ù„Øª Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© Ø¨Ù†Ø¬Ø§Ø­!")

