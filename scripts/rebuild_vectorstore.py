"""
rebuild_vectorstore.py
Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©
"""

import sys
from pathlib import Path

BASE_DIR = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(BASE_DIR))

from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
import shutil
import os
import re

from config import Settings

print("âš™ï¸ Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© Ø¨Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø­Ø³Ù‘Ù†Ø©...")

# Ø­Ø°Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
if Settings.CHROMA_DB_DIR.exists():
    print("ğŸ—‘ï¸  Ø­Ø°Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©...")
    shutil.rmtree(Settings.CHROMA_DB_DIR)

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
Settings.CHROMA_DB_DIR.mkdir(parents=True, exist_ok=True)

# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Øµ
print("ğŸ“„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Øµ...")
loader = TextLoader(str(Settings.DATA_FILE), encoding="utf-8")
documents = loader.load()
print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(documents)} Ù…Ø³ØªÙ†Ø¯")

# 2. Ø¥Ø«Ø±Ø§Ø¡ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø¨Ù€ metadata Ø°ÙƒÙŠ
print("ğŸ·ï¸  Ø¬Ø§Ø±ÙŠ Ø¥Ø«Ø±Ø§Ø¡ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø¨Ù€ metadata Ø°ÙƒÙŠ...")
full_text = documents[0].page_content

sections = re.split(r"(?=\n===\s.+?\s===)", full_text)
enhanced_docs = []

# Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ù„Ù„ÙÙ„ØªØ±Ø© Ø§Ù„Ø°ÙƒÙŠØ© Ù…Ø¹ ØªÙ…ÙŠÙŠØ² Ø¯Ù‚ÙŠÙ‚ Ø¨ÙŠÙ† Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨Ø§Ù‚Ø§Øª
section_mapping = {
    "Ù…Ù„Ø®Øµ ØªÙ†ÙÙŠØ°ÙŠ": {"section": "Ù…Ù„Ø®Øµ", "category": "Ø¹Ø§Ù…", "package_type": None, "keywords": ["Ø¨Ø§Ù‚Ø§Øª", "Ø£Ø³Ø¹Ø§Ø±", "Ø¯Ø¹Ù…", "ØªÙˆØ§ØµÙ„"]},
    "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø±ÙƒØ©": {"section": "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø±ÙƒØ©", "category": "Ø¹Ø§Ù…", "package_type": None, "keywords": ["Ø´Ø±ÙƒØ©", "ØªØ§Ø±ÙŠØ®", "ØªØ£Ø³ÙŠØ³", "Ø´Ù…Ø³", "ØªÙŠÙ„ÙŠÙƒÙˆÙ…"]},
    "Ø§Ù„Ù…Ù‡Ù…Ø© ÙˆØ§Ù„Ù‚ÙŠÙ…": {"section": "Ø§Ù„Ù…Ù‡Ù…Ø© ÙˆØ§Ù„Ù‚ÙŠÙ…", "category": "Ø¹Ø§Ù…", "package_type": None, "keywords": ["Ù…Ù‡Ù…Ø©", "Ù‚ÙŠÙ…", "Ø±Ø¤ÙŠØ©"]},
    "Ø§Ù„Ø¨Ø§Ù‚Ø§Øª - Ø§Ù„ÙƒØ§Ø¨Ù„ Ø§Ù„Ø¶ÙˆØ¦ÙŠ": {"section": "Ø¨Ø§Ù‚Ø§Øª", "category": "Ø¨Ø§Ù‚Ø§Øª", "package_type": "fiber", "keywords": ["ÙØ§ÙŠØ¨Ø±", "FTTH", "ÙƒØ§Ø¨Ù„ Ø¶ÙˆØ¦ÙŠ", "Ø£Ù„ÙŠØ§Ù", "Ø³Ø¹Ø±", "Ø¨Ø§Ù‚Ø©", "35", "50", "75", "150"]},
    "Ø§Ù„Ø¨Ø§Ù‚Ø§Øª - Ø§Ù„ÙˆØ§ÙŠØ±Ù„Ø³": {"section": "Ø¨Ø§Ù‚Ø§Øª", "category": "Ø¨Ø§Ù‚Ø§Øª", "package_type": "wireless", "keywords": ["ÙˆØ§ÙŠØ±Ù„Ø³", "WiFi", "wireless", "Star", "Sun", "Neptune", "Galaxy"]},
    "Ø¨Ø§Ù‚Ø§Øª Ø®Ø¯Ù…Ø©": {"section": "Ø¹Ø±ÙˆØ¶", "category": "Ø¹Ø±ÙˆØ¶", "package_type": None, "keywords": ["Ù…Ù†ØµØ©", "ØªØ±ÙÙŠÙ‡", "Ø¨Ø«"]},
    "Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©": {"section": "Ø®Ø¯Ù…Ø§Øª", "category": "Ø®Ø¯Ù…Ø§Øª", "package_type": None, "keywords": ["Ø®Ø¯Ù…Ø©", "Ø¥Ù†ØªØ±Ù†Øª", "FTTH", "WiFi"]},
    "Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„ØªØºØ·ÙŠØ©": {"section": "ØªØºØ·ÙŠØ©", "category": "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª", "package_type": None, "keywords": ["ØªØºØ·ÙŠØ©", "Ø¨ØºØ¯Ø§Ø¯", "Ø¯ÙŠØ§Ù„Ù‰", "Ø¨Ø§Ø¨Ù„", "Ø§Ù„Ù…Ø³ÙŠØ¨", "Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©", "Ø³Ø¯Ø© Ø§Ù„Ù‡Ù†Ø¯ÙŠØ©", "ÙØ±Ø¹"]},
    "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙˆØ§ØµÙ„": {"section": "ØªÙˆØ§ØµÙ„", "category": "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª", "package_type": None, "keywords": ["Ù‡Ø§ØªÙ", "Ø¨Ø±ÙŠØ¯", "ÙˆØ§ØªØ³Ø§Ø¨", "6449", "info@"]},
    "Ù„Ù…Ø§Ø°Ø§ ØªØ®ØªØ§Ø±": {"section": "Ù…Ø²Ø§ÙŠØ§", "category": "Ø¹Ø§Ù…", "package_type": None, "keywords": ["Ø¯Ø¹Ù…", "24", "Ø£Ù…Ù†", "ÙˆØµÙˆÙ„"]},
    "Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„ÙÙ†ÙŠ": {"section": "Ø¯Ø¹Ù…", "category": "Ø®Ø¯Ù…Ø§Øª", "package_type": None, "keywords": ["Ø¯Ø¹Ù…", "ÙÙ†ÙŠ", "24", "Ù…Ø³Ø§Ø¹Ø¯Ø©", "Ù…Ø´Ø§ÙƒÙ„"]},
    "Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©": {"section": "FAQ", "category": "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª", "package_type": None, "keywords": ["Ø³Ø¤Ø§Ù„", "Ø¬ÙˆØ§Ø¨", "Ø´Ø§Ø¦Ø¹"]},
    "Ø·Ø±Ù‚ Ø§Ù„Ø¯ÙØ¹": {"section": "Ø¯ÙØ¹", "category": "Ø®Ø¯Ù…Ø§Øª", "package_type": None, "keywords": ["Ø¯ÙØ¹", "ØªØ¬Ø¯ÙŠØ¯", "Ø¨Ø§Ù‚Ø©"]},
    "ØªØ¬Ø¯ÙŠØ¯ Ø§Ù„Ø¨Ø§Ù‚Ø§Øª": {"section": "ØªØ¬Ø¯ÙŠØ¯", "category": "Ø®Ø¯Ù…Ø§Øª", "package_type": None, "keywords": ["ØªØ¬Ø¯ÙŠØ¯", "Ø¨Ø§Ù‚Ø©", "Ø¯ÙØ¹"]},
}

def detect_section_category(section_name: str, content: str) -> dict:
    """Ø§ÙƒØªØ´Ø§Ù ÙØ¦Ø© Ø§Ù„Ù‚Ø³Ù… ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ø¹ ØªÙ…ÙŠÙŠØ² Ø¯Ù‚ÙŠÙ‚ Ø¨ÙŠÙ† Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨Ø§Ù‚Ø§Øª"""
    content_lower = content.lower()
    
    # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø£Ù‚Ø³Ø§Ù…
    for key, info in section_mapping.items():
        if key in section_name:
            return info
    
    # Ø§ÙƒØªØ´Ø§Ù ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    category = "Ø¹Ø§Ù…"
    package_type = None
    keywords = []
    
    # ØªÙ…ÙŠÙŠØ² Ø¯Ù‚ÙŠÙ‚ Ø¨ÙŠÙ† Ø¨Ø§Ù‚Ø§Øª Ø§Ù„ÙØ§ÙŠØ¨Ø± ÙˆØ§Ù„ÙˆØ§ÙŠØ±Ù„Ø³
    if any(kw in content_lower for kw in ["ÙØ§ÙŠØ¨Ø±", "ftth", "ÙƒØ§Ø¨Ù„ Ø¶ÙˆØ¦ÙŠ", "Ø£Ù„ÙŠØ§Ù"]):
        category = "Ø¨Ø§Ù‚Ø§Øª"
        package_type = "fiber"
        keywords = ["ÙØ§ÙŠØ¨Ø±", "FTTH", "ÙƒØ§Ø¨Ù„ Ø¶ÙˆØ¦ÙŠ", "35", "50", "75", "150"]
    elif any(kw in content_lower for kw in ["ÙˆØ§ÙŠØ±Ù„Ø³", "wireless", "wifi", "star", "sun", "neptune", "galaxy"]):
        category = "Ø¨Ø§Ù‚Ø§Øª"
        package_type = "wireless"
        keywords = ["ÙˆØ§ÙŠØ±Ù„Ø³", "WiFi", "wireless", "Star", "Sun", "Neptune", "Galaxy"]
    elif any(kw in content_lower for kw in ["Ø¨Ø§Ù‚Ø©", "Ø³Ø¹Ø±", "Ø¯ÙŠÙ†Ø§Ø±"]):
        category = "Ø¨Ø§Ù‚Ø§Øª"
        keywords = ["Ø¨Ø§Ù‚Ø©", "Ø³Ø¹Ø±", "Ø¨Ø§Ù‚Ø§Øª"]
    elif any(kw in content_lower for kw in ["Ø¯Ø¹Ù…", "ÙÙ†ÙŠ", "24", "Ù…Ø³Ø§Ø¹Ø¯Ø©"]):
        category = "Ø®Ø¯Ù…Ø§Øª"
        keywords = ["Ø¯Ø¹Ù…", "ÙÙ†ÙŠ"]
    elif any(kw in content_lower for kw in ["ØªØºØ·ÙŠØ©", "Ø¨ØºØ¯Ø§Ø¯", "Ø¯ÙŠØ§Ù„Ù‰", "Ø¨Ø§Ø¨Ù„"]):
        category = "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"
        keywords = ["ØªØºØ·ÙŠØ©", "Ù…Ù†Ø·Ù‚Ø©", "Ø¨ØºØ¯Ø§Ø¯", "Ø¯ÙŠØ§Ù„Ù‰", "Ø¨Ø§Ø¨Ù„"]
    elif any(kw in content_lower for kw in ["Ù‡Ø§ØªÙ", "Ø¨Ø±ÙŠØ¯", "ÙˆØ§ØªØ³Ø§Ø¨", "6449"]):
        category = "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"
        keywords = ["ØªÙˆØ§ØµÙ„", "Ù‡Ø§ØªÙ"]
    elif any(kw in content_lower for kw in ["Ø´Ø±ÙƒØ©", "Ø´Ù…Ø³", "ØªÙŠÙ„ÙŠÙƒÙˆÙ…", "Ù…Ù† Ù†Ø­Ù†"]):
        category = "Ø¹Ø§Ù…"
        keywords = ["Ø´Ø±ÙƒØ©", "Ø´Ù…Ø³", "ØªÙŠÙ„ÙŠÙƒÙˆÙ…"]
    
    return {
        "section": section_name,
        "category": category,
        "package_type": package_type,
        "keywords": keywords
    }

for section in sections:
    if not section.strip():
        continue

    header_match = re.search(r"===\s*(.+?)\s*===", section)
    section_name = header_match.group(1).strip() if header_match else "Ø¹Ø§Ù…"

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ metadata Ù…Ù† Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª [Ø§Ù„Ù‚Ø³Ù…: ...]
    metadata_tags = {}
    tag_match = re.search(r'\[Ø§Ù„Ù‚Ø³Ù…:\s*(.+?)\]', section)
    if tag_match:
        metadata_tags["tag_section"] = tag_match.group(1).strip()
    
    type_match = re.search(r'\[Ø§Ù„Ù†ÙˆØ¹:\s*(.+?)\]', section)
    if type_match:
        metadata_tags["tag_type"] = type_match.group(1).strip()
    
    # Ø§ÙƒØªØ´Ø§Ù ÙØ¦Ø© Ø§Ù„Ù‚Ø³Ù…
    section_info = detect_section_category(section_name, section)
    
    # ÙÙ„ØªØ±Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
    if "Ø£Ø®Ø¨Ø§Ø± ÙˆÙ…Ù‚Ø§Ù„Ø§Øª" in section_name or "Ø´Ø§Ø±ÙƒÙ†Ø§ ÙÙŠ ÙØ¹Ø§Ù„ÙŠØ©" in section:
        lines = section.split("\n")
        brief_section = "\n".join([lines[0], *[line for line in lines[1:4] if line.strip()]])
        content = brief_section
    else:
        content = section

    # Ø¥Ù†Ø´Ø§Ø¡ metadata Ø´Ø§Ù…Ù„ Ù…Ø¹ ØªÙ…ÙŠÙŠØ² Ø¯Ù‚ÙŠÙ‚ Ø¨ÙŠÙ† Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨Ø§Ù‚Ø§Øª
    metadata = {
        "section": section_info["section"],
        "category": section_info["category"],
        "keywords": ", ".join(section_info["keywords"]) if section_info["keywords"] else "",
        **metadata_tags
    }
    
    # Ø¥Ø¶Ø§ÙØ© package_type Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹ (Ù„Ù„ØªÙ…ÙŠÙŠØ² Ø¨ÙŠÙ† Ø§Ù„ÙØ§ÙŠØ¨Ø± ÙˆØ§Ù„ÙˆØ§ÙŠØ±Ù„Ø³)
    if "package_type" in section_info and section_info["package_type"]:
        metadata["package_type"] = section_info["package_type"]

    enhanced_docs.append({
        "page_content": content.strip(),
        "metadata": metadata
    })

print(f"âœ… ØªÙ… ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ {len(enhanced_docs)} Ù‚Ø³Ù… Ù…Ø¹ metadata Ø°ÙƒÙŠ")

# 3. ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ù†Øµ - Ù…Ø­Ø³Ù‘Ù†Ø© Ù„Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¹Ø§Ù…Ø©
print("âœ‚ï¸  Ø¬Ø§Ø±ÙŠ ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ù†Øµ...")
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,  # Ø­Ø¬Ù… Ø£ÙƒØ¨Ø± Ù„ÙŠØ´Ù…Ù„ Ù‚Ø³Ù… Ø§Ù„Ø¨Ø§Ù‚Ø§Øª ÙƒØ§Ù…Ù„Ø§Ù‹
    chunk_overlap=100,
    separators=[
        "\n=== ",           # ÙØµÙ„ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© (Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„Ø£ÙˆÙ„Ù‰)
        "\n---\n",          # ÙØµÙ„ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„ÙØ±Ø¹ÙŠØ©
        "\n\n",             # ÙÙ‚Ø±Ø§Øª
        "\n",               # Ø£Ø³Ø·Ø±
        ". ",               # Ø¬Ù…Ù„
        "ØŒ ",               # ÙÙˆØ§ØµÙ„ Ø¹Ø±Ø¨ÙŠØ©
        " "                 # ÙƒÙ„Ù…Ø§Øª
    ],
    length_function=len,
    is_separator_regex=False
)

final_chunks = []
for doc in enhanced_docs:
    chunks = text_splitter.split_text(doc["page_content"])
    for chunk in chunks:
        if chunk.strip():
            final_chunks.append({
                "page_content": chunk.strip(),
                "metadata": doc["metadata"]
            })

print(f"âœ… ØªÙ… ØªÙˆÙ„ÙŠØ¯ {len(final_chunks)} Ø¬Ø²Ø¡Ù‹Ø§ Ø¬Ø§Ù‡Ø²Ù‹Ø§ Ù„Ù„ØªØ¶Ù…ÙŠÙ†")

# 4. ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†
print("ğŸ”¢ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†...")
embeddings = HuggingFaceEmbeddings(
    model_name=Settings.EMBEDDING_MODEL,
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)

test_vec = embeddings.embed_query("ØªØ¬Ø±Ø¨Ø©")
print(f"âœ… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø¬Ø§Ù‡Ø²! Ø·ÙˆÙ„ Ø§Ù„Ù…ØªØ¬Ù‡: {len(test_vec)}")

# 5. Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø³ØªÙ†Ø¯Ø§Øª LangChain
langchain_docs = [
    Document(page_content=item["page_content"], metadata=item["metadata"])
    for item in final_chunks
]

# 6. Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ¬Ù‡Ø©
print("ğŸ’¾ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©...")
vectorstore = Chroma.from_documents(
    documents=langchain_docs,
    embedding=embeddings,
    persist_directory=str(Settings.CHROMA_DB_DIR)
)

vectorstore.persist()
print(f"âœ… ØªÙ… Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© ÙÙŠ '{Settings.CHROMA_DB_DIR}'")

# 7. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹
print("\nğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª...")
# Ø§Ø³ØªØ®Ø¯Ø§Ù… retriever Ø¨Ø¯ÙˆÙ† threshold Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± (Ø£ÙƒØ«Ø± Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©)
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}
)

test_queries = [
    "Ù…Ø§ Ø³Ø¹Ø± Ø¨Ø§Ù‚Ø© ÙØ§ÙŠØ¨Ø± 75ØŸ",
    "Ù‡Ù„ Ù„Ø¯ÙŠÙƒÙ… Ø¯Ø¹Ù… ÙÙ†ÙŠ 24 Ø³Ø§Ø¹Ø©ØŸ",
    "Ù…Ø§ Ù‡ÙŠ Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„ØªØºØ·ÙŠØ©ØŸ",
    "Ù…Ù† Ù‡Ù… Ø´Ø±ÙƒØ§Ø¤ÙƒÙ…ØŸ"
]

for query in test_queries:
    print(f"\nâ“ Ø§Ù„Ø³Ø¤Ø§Ù„: {query}")
    results = retriever.invoke(query)
    print(f"   ğŸ“Š ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ {len(results)} Ù†ØªÙŠØ¬Ø©")
    if results:
        print(f"   ğŸ“‚ Ø§Ù„Ù‚Ø³Ù…: {results[0].metadata.get('section', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}")
        print(f"   ğŸ“„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {results[0].page_content[:120]}...")

print("\nâœ… Ø§ÙƒØªÙ…Ù„Øª Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø© Ø¨Ù†Ø¬Ø§Ø­!")

