"""
rag_engine.py
Ù…Ø­Ø±Ùƒ RAG (Retrieval-Augmented Generation) Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡
Ø­ÙˆÙ„ Ø´Ø±ÙƒØ© Ø§Ù„Ø´Ù…Ø³ ØªÙŠÙ„ÙŠÙƒÙˆÙ… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©.
"""

import logging
import sys
from pathlib import Path

# Ø¥Ø¶Ø§ÙØ© Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¥Ù„Ù‰ Python path
BASE_DIR = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(BASE_DIR))

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from config import Settings

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
logging.basicConfig(level=getattr(logging, Settings.LOG_LEVEL))
logger = logging.getLogger(__name__)

# === ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… ===
try:
    logger.info("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†...")
    embeddings = HuggingFaceEmbeddings(
        model_name=Settings.EMBEDDING_MODEL,
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True}
    )
    
    logger.info("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©...")
    vectorstore = Chroma(
        persist_directory=str(Settings.CHROMA_DB_DIR),
        embedding_function=embeddings
    )
    
    # Retriever Ù…Ø­Ø³Ù‘Ù†
    retriever = vectorstore.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={
            "k": Settings.RETRIEVER_K,
            "score_threshold": Settings.RETRIEVER_SCORE_THRESHOLD
        }
    )
    
    # Retriever Ø§Ø­ØªÙŠØ§Ø·ÙŠ
    fallback_retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 5}
    )
    
    logger.info("Ø¬Ø§Ø±ÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ...")
    llm = ChatOllama(
        model=Settings.LLM_MODEL,
        temperature=Settings.LLM_TEMPERATURE,
        num_ctx=Settings.LLM_CONTEXT_SIZE
    )
    
    logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ù†Ø¬Ø§Ø­")
    
except Exception as e:
    logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù…: {str(e)}")
    raise

# Ù‚Ø§Ù„Ø¨ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù‘Ù†
prompt = ChatPromptTemplate.from_messages([
    ("system", """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ø§Ø­ØªØ±Ø§ÙÙŠ Ù„Ø´Ø±ÙƒØ© "Ø§Ù„Ø´Ù…Ø³ ØªÙŠÙ„ÙŠÙƒÙˆÙ…". Ù‚ÙˆØ§Ø¹Ø¯Ùƒ Ø§Ù„ØµØ§Ø±Ù…Ø©:

**Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:**
1. **Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·** - Ù…Ù…Ù†ÙˆØ¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙŠ Ù„ØºØ© Ø£Ø®Ø±Ù‰.
2. **Ø§Ø³ØªØ®Ù„Øµ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚** - Ù…Ù…Ù†ÙˆØ¹ Ø§Ù„Ø§Ø®ØªÙ„Ø§Ù‚ Ø£Ùˆ Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ù…Ù† Ø®Ø§Ø±Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚.
3. **Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚**ØŒ Ù‚Ù„ Ø¨ÙˆØ¶ÙˆØ­: "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§ØªÙ†Ø§ Ø§Ù„Ø­Ø§Ù„ÙŠØ©."

**Ø¹Ù†Ø¯ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø¨Ø§Ù‚Ø§Øª Ø£Ùˆ Ø§Ù„Ø£Ø³Ø¹Ø§Ø±:**
- Ø§Ø°ÙƒØ± **Ø¬Ù…ÙŠØ¹** Ø§Ù„Ø¨Ø§Ù‚Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚
- Ù„ÙƒÙ„ Ø¨Ø§Ù‚Ø©: Ø§Ù„Ø§Ø³Ù…ØŒ Ø§Ù„Ø³Ø¹Ø± (Ø¨Ø§Ù„Ø¯ÙŠÙ†Ø§Ø± Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠ)ØŒ Ø§Ù„Ù†ÙˆØ¹ (ÙØ§ÙŠØ¨Ø±/ÙˆØ§ÙŠØ±Ù„Ø³/Ù…Ù†ØµØ©)ØŒ Ø§Ù„Ø³Ø±Ø¹Ø© (Ø¥Ù† ÙˆØ¬Ø¯Øª)ØŒ ÙˆØµÙ Ù…ÙˆØ¬Ø²
- Ù†Ø¸Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù‚Ø§Ø· Ø£Ùˆ Ø¬Ø¯Ø§ÙˆÙ„

**Ø¹Ù†Ø¯ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø®Ø¯Ù…Ø§Øª:**
- Ø§Ø°ÙƒØ± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚
- Ø§Ø°ÙƒØ± Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ù‡Ù…Ø© (Ù…Ø«Ù„: 70,000 Ø®Ø· ÙƒØ§Ø¨Ù„ Ø¶ÙˆØ¦ÙŠ)

**Ø¹Ù†Ø¯ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„ØªÙˆØ§ØµÙ„:**
- Ø§Ø°ÙƒØ±: Ø§Ù„Ù‡Ø§ØªÙ (6449)ØŒ Ø§Ù„Ø¨Ø±ÙŠØ¯ (info@shams-tele.com)ØŒ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ØŒ ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ

**Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:**
- Ø§Ø­ØªØ±Ø§ÙÙŠØŒ ÙˆØ¯ÙˆØ¯ØŒ ÙˆÙˆØ§Ø¶Ø­
- Ø§Ø³ØªØ®Ø¯Ù… Ø¹Ù†Ø§ÙˆÙŠÙ† ÙØ±Ø¹ÙŠØ© Ø£Ùˆ Ù†Ù‚Ø§Ø· Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
- Ù„Ø§ ØªÙƒØ±Ø± Ù†ÙØ³ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø©
- ÙƒÙ† Ù…ÙˆØ¬Ø²Ù‹Ø§ Ù„ÙƒÙ† Ø´Ø§Ù…Ù„Ù‹Ø§

**Ù…Ù…Ù†ÙˆØ¹ ØªÙ…Ø§Ù…Ù‹Ø§:**
- Ø§Ù„Ø§Ø®ØªÙ„Ø§Ù‚ Ø£Ùˆ Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ù…Ù† Ø®Ø§Ø±Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚
- Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„ØºØ§Øª ØºÙŠØ± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
- Ø¥Ø¹Ø·Ø§Ø¡ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚
- Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ù…ÙØ±Ø·

**ØªØ£ÙƒØ¯ Ù…Ù†:**
- Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·
- Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙÙ‚Ø·
- Ø§Ù„Ø¯Ù‚Ø© ÙÙŠ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ø£Ø³Ù…Ø§Ø¡
"""),
    ("human", """**Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…ØªØ§Ø­ (Ø§Ø³ØªØ®Ø¯Ù…Ù‡ ÙÙ‚Ø· ÙƒÙ…ØµØ¯Ø± Ù„Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª):**
{context}

**Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¹Ù…ÙŠÙ„:**
{input}

**Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ Ø£Ø¹Ù„Ø§Ù‡ Ø­ØµØ±ÙŠÙ‹Ø§:**""")
])


def filter_and_deduplicate_docs(docs) -> str:
    """ØªØµÙÙŠØ© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØ¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±"""
    if not docs:
        return "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ©."
    
    filtered = []
    for doc in docs:
        content = doc.page_content.strip()
        if not content:
            continue
        
        # ØªØµÙÙŠØ© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ØºÙŠØ± Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
        if any(term in content for term in ["Ø´Ø§Ø±ÙƒÙ†Ø§ ÙÙŠ ÙØ¹Ø§Ù„ÙŠØ©", "ÙˆØ±Ø´Ø© Ø¹Ù…Ù„", "Ø±Ø¹Ø§Ø©", "Ø­Ø¯Ø«", "Ù…Ø¤ØªÙ…Ø±"]):
            if len(content) > 300:
                first_line = content.split("\n")[0]
                if any(keyword in first_line for keyword in ["Ø´Ù…Ø³", "Ø¨Ø§Ù‚Ø©", "Ø³Ø¹Ø±", "Ø¯ÙŠÙ†Ø§Ø±", "FTTH", "WiFi"]):
                    filtered.append(first_line)
                continue
        filtered.append(content)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±
    unique_contents = []
    seen = set()
    for text in filtered:
        text_lower = text.lower().strip()
        key = text_lower[:100] if len(text_lower) > 100 else text_lower
        if key not in seen:
            seen.add(key)
            unique_contents.append(text)
    
    context = "\n---\n".join(unique_contents)
    return context[:2000]


def expand_query(question: str) -> str:
    """ØªÙˆØ³ÙŠØ¹ Ø°ÙƒÙŠ Ù„Ù„Ø³Ø¤Ø§Ù„"""
    q = question.strip().lower()
    original = question.strip()
    
    if any(w in q for w in ["Ø§Ù„Ø³Ø¹Ø±", "Ø³Ø¹Ø±", "ÙƒÙ…", "Ø¯ÙŠÙ†Ø§Ø±", "ØªÙƒÙ„ÙØ©", "Ø«Ù…Ù†"]):
        return f"{original} Ø¨Ø§Ù‚Ø§Øª Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ø§Ù„Ø£Ø³Ø¹Ø§Ø± FTTH WiFi Ø¯ÙŠÙ†Ø§Ø± Ø¹Ø±Ø§Ù‚ÙŠ ÙØ§ÙŠØ¨Ø± ÙˆØ§ÙŠØ±Ù„Ø³"
    
    if any(w in q for w in ["Ø¨Ø§Ù‚Ø©", "Ø§Ø´ØªØ±Ø§Ùƒ", "Ø§Ù„Ø¨Ø§Ù‚Ø§Øª", "Ø¨Ø§Ù‚Ø§Øª", "Ø®Ø·Ø©"]):
        return f"{original} Ø¨Ø§Ù‚Ø§Øª Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª FTTH WiFi Ø§Ù„Ù…Ù†ØµØ© ÙØ§ÙŠØ¨Ø± Star Sun Neptune Galaxy Star"
    
    if any(w in q for w in ["ØªØºØ·ÙŠØ©", "Ù…Ù†Ø·Ù‚Ø©", "Ø£ÙŠÙ†", "ÙØ±Ø¹", "Ù…ÙƒØ§Ù†"]):
        return f"{original} ØªØºØ·ÙŠØ© Ø¨ØºØ¯Ø§Ø¯ Ø¯ÙŠØ§Ù„Ù‰ Ø¨Ø§Ø¨Ù„ Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø§Øª ÙØ±Ø¹ Ù…ÙˆÙ‚Ø¹"
    
    if any(w in q for w in ["Ø¯Ø¹Ù…", "Ù…Ø³Ø§Ø¹Ø¯Ù‡", "24", "Ø®Ø¯Ù…Ø©", "Ø§ØªØµØ§Ù„"]):
        return f"{original} Ø¯Ø¹Ù… ÙÙ†ÙŠ 24/7 Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ ØªÙˆØ§ØµÙ„"
    
    return original


def is_arabic_text(text: str) -> bool:
    """ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ Ø¹Ø±Ø¨ÙŠÙ‹Ø§"""
    if not text or not text.strip():
        return False
    
    arabic_chars = sum(1 for char in text if '\u0600' <= char <= '\u06FF' or char in 'ØŒØ›ØŸ')
    total_chars = len([c for c in text if c.isalpha() or c in 'ØŒØ›ØŸ'])
    
    if total_chars == 0:
        return False
    
    arabic_ratio = arabic_chars / total_chars if total_chars > 0 else 0
    return arabic_ratio >= 0.3


def validate_answer(answer: str, context: str) -> tuple[bool, str]:
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©"""
    if not answer or len(answer.strip()) < 10:
        return False, "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ù‹Ø§"
    
    if not is_arabic_text(answer):
        return False, "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù„ÙŠØ³Øª Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
    
    words = answer.split()
    if len(set(words)) < len(words) * 0.3:
        return False, "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªÙƒØ±Ø§Ø± Ù…ÙØ±Ø·"
    
    return True, "ØµØ­ÙŠØ­Ø©"


# Ø³Ù„Ø³Ù„Ø© RAG
rag_chain = (
    {"context": retriever | filter_and_deduplicate_docs, "input": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)


def get_answer(question: str, max_retries: int = 2) -> str:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø©"""
    if not question or not question.strip():
        return "Ù…Ø±Ø­Ø¨Ø§Ù‹! ğŸŒ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ Ø§Ø³Ø£Ù„Ù†ÙŠ Ø¹Ù† Ø¨Ø§Ù‚Ø§Øª Ø§Ù„Ø¥Ù†ØªØ±Ù†ØªØŒ Ø§Ù„Ø£Ø³Ø¹Ø§Ø±ØŒ Ø§Ù„ØªØºØ·ÙŠØ©ØŒ Ø£Ùˆ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø´Ù…Ø³ ØªÙŠÙ„ÙŠÙƒÙˆÙ…."

    clean_q = question.strip()
    logger.info(f"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: '{clean_q}'")

    if not is_arabic_text(clean_q):
        logger.warning(f"Ø§Ù„Ø³Ø¤Ø§Ù„ Ù‚Ø¯ Ù„Ø§ ÙŠÙƒÙˆÙ† Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: '{clean_q}'")

    expanded = expand_query(clean_q)
    if expanded != clean_q:
        logger.debug(f"Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…ÙˆØ³Ù‘Ø¹: {expanded}")

    context_used = ""
    
    try:
        docs = retriever.invoke(expanded)
        logger.info(f"ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ {len(docs)} Ù…Ø³ØªÙ†Ø¯(Ø§Øª)")

        if not docs:
            fallback_queries = [
                clean_q,
                " ".join([w for w in clean_q.split() if len(w) > 2]),
                "Ø¨Ø§Ù‚Ø§Øª Ø£Ø³Ø¹Ø§Ø± Ø¯Ø¹Ù… ØªÙˆØ§ØµÙ„"
            ]
            for fq in fallback_queries:
                docs = retriever.invoke(fq)
                if docs:
                    logger.info(f"ØªÙ… Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ: {fq}")
                    break
            
            if not docs:
                docs = fallback_retriever.invoke(clean_q)
                logger.info(f"ØªÙ… Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… retriever Ø§Ø­ØªÙŠØ§Ø·ÙŠ: {len(docs)} Ù…Ø³ØªÙ†Ø¯(Ø§Øª)")

        context_used = filter_and_deduplicate_docs(docs)
        
        response = None
        for attempt in range(max_retries + 1):
            try:
                response = rag_chain.invoke(clean_q)
                response = response.strip()
                
                is_valid, validation_msg = validate_answer(response, context_used)
                if is_valid:
                    logger.info(f"ØªÙ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1})")
                    break
                else:
                    logger.warning(f"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØºÙŠØ± ØµØ­ÙŠØ­Ø©: {validation_msg} (Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1})")
                    if attempt < max_retries:
                        continue
                    else:
                        response = "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§ØªÙ†Ø§ Ø§Ù„Ø­Ø§Ù„ÙŠØ©.\n\nÙ‡Ù„ ØªØ±ÙŠØ¯ Ù…Ø¹Ø±ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø¨Ø§Ù‚Ø§ØªÙ†Ø§ Ø£Ùˆ Ø®Ø¯Ù…Ø§ØªÙ†Ø§ØŸ ğŸ˜Š"
            except Exception as e:
                logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}: {e}")
                if attempt < max_retries:
                    continue
                else:
                    raise
        
        if not response:
            response = "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§ØªÙ†Ø§ Ø§Ù„Ø­Ø§Ù„ÙŠØ©.\n\nÙ‡Ù„ ØªØ±ÙŠØ¯ Ù…Ø¹Ø±ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø¨Ø§Ù‚Ø§ØªÙ†Ø§ Ø£Ùˆ Ø®Ø¯Ù…Ø§ØªÙ†Ø§ØŸ ğŸ˜Š"
        
        import re
        response = re.sub(r'^(Answer|Response|Reply):\s*', '', response, flags=re.IGNORECASE)
        response = response.strip()
        
        if not is_arabic_text(response):
            logger.warning("Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù‚Ø¯ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Øµ ØºÙŠØ± Ø¹Ø±Ø¨ÙŠØŒ Ø³ÙŠØªÙ… Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©...")
            try:
                response = rag_chain.invoke(f"{clean_q}\n\nØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·.")
                response = response.strip()
            except:
                pass
        
        return response

    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {e}", exc_info=True)
        return "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ù‹Ø§ Ø£Ùˆ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù†Ø§ Ø¹Ù„Ù‰ 6449."

