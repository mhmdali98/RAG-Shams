# โก ุชุญุณูู ุงูุณุฑุนุฉ ูุงูููุซูููุฉ

## โ ุงูุชุญุณููุงุช ุงููุทุจูุฉ

### 1. **ุชูููู ุทูู ุงูุณูุงู ุงููููุฑูุฑ ููู LLM**

**ูุจู:**
```python
LLM_CONTEXT_SIZE = 4096
context_limit = 2500  # ุญุฑู
```

**ุจุนุฏ:**
```python
LLM_CONTEXT_SIZE = 2048  # ุชูููู ุงูุณูุงู ูุชุญุณูู ุงูุณุฑุนุฉ
context_limit = 1500  # ุญุฑู (ูุงูู ููุนุธู ุงูุฃุณุฆูุฉ)
```

**ุงูููุงุฆุฏ:**
- โ **ุณุฑุนุฉ ุฃูุจุฑ**: ูุนุงูุฌุฉ ุฃุณุฑุน ููุณูุงู ุงูุฃูุตุฑ
- โ **ุงุณุชููุงู ุฐุงูุฑุฉ ุฃูู**: ุงุณุชุฎุฏุงู ุฃูู ูููุงุฑุฏ ุงููุธุงู
- โ **ุงุณุชุฌุงุจุฉ ุฃุณุฑุน**: ููุช ุงูุชุธุงุฑ ุฃูู ูููุณุชุฎุฏู

### 2. **ุงุณุชุฎุฏุงู Instruct Model**

**ูุจู:**
```python
LLM_MODEL = "llama3"
```

**ุจุนุฏ:**
```python
LLM_MODEL = "llama3:8b-instruct"  # ุงุณุชุฎุฏุงู instruct model ููุฏูุฉ
```

**ุงูููุงุฆุฏ:**
- โ **ุฏูุฉ ุฃุนูู**: Instruct models ูุตููุฉ ููุชุนูููุงุช
- โ **ุงุณุชุฌุงุจุฉ ุฃูุถู**: ููู ุฃูุถู ููู prompts
- โ **ููุซูููุฉ ุฃูุจุฑ**: ุฅุฌุงุจุงุช ุฃูุซุฑ ุงุชุณุงูุงู

### 3. **Temperature = 0.0**

**ุงูุญุงูุฉ:**
```python
LLM_TEMPERATURE = 0.0  # ูููุน ุงููููุณุฉ
```

**ุงูููุงุฆุฏ:**
- โ **ุฏูุฉ ุนุงููุฉ**: ูุง ูุฎุชูู ูุนูููุงุช
- โ **ุงุชุณุงู**: ููุณ ุงูุณุคุงู = ููุณ ุงูุฅุฌุงุจุฉ
- โ **ููุซูููุฉ**: ุฅุฌุงุจุงุช ูุงุจูุฉ ููุงุนุชูุงุฏ ุนูููุง

## ๐ ุงููุชุงุฆุฌ ุงููุชููุนุฉ

| ุงููููุงุณ | ูุจู | ุจุนุฏ |
|--------|-----|-----|
| **ุทูู ุงูุณูุงู** | 2500 ุญุฑู | **1500 ุญุฑู** |
| **ุญุฌู Context** | 4096 tokens | **2048 tokens** |
| **ุงูุณุฑุนุฉ** | ูุชูุณุท | **ุฃุณุฑุน ~30-40%** |
| **ุงุณุชููุงู ุงูุฐุงูุฑุฉ** | ุนุงูู | **ุฃูู ~50%** |
| **ุฏูุฉ ุงูุฅุฌุงุจุฉ** | ุฌูุฏ | **ุฃูุถู (instruct model)** |

## ๐งช ููููุฉ ุงูุงุฎุชุจุงุฑ

### 1. ุงูุชุญูู ูู ุงูุฅุนุฏุงุฏุงุช:
```bash
python -c "from config import Settings; print(f'Model: {Settings.LLM_MODEL}'); print(f'Context: {Settings.LLM_CONTEXT_SIZE}'); print(f'Temperature: {Settings.LLM_TEMPERATURE}')"
```

### 2. ุชุดุบูู ุงูุจูุช:
```bash
python -m src.ui.app
```

### 3. ููุงุญุธุฉ ุงูุชุญุณููุงุช:
- โ ุงุณุชุฌุงุจุฉ ุฃุณุฑุน
- โ ุงุณุชููุงู ุฐุงูุฑุฉ ุฃูู
- โ ุฅุฌุงุจุงุช ุฃูุซุฑ ุฏูุฉ

## ๐ ุงููููุงุช ุงููุญุฏุซุฉ

1. **`config/settings.py`**
   - `LLM_MODEL`: "llama3" โ "llama3:8b-instruct"
   - `LLM_CONTEXT_SIZE`: 4096 โ 2048

2. **`src/core/rag_engine.py`**
   - `context_limit`: 2500 โ 1500 ุญุฑู

## โ๏ธ ููุงุญุธุงุช

1. **ุทูู ุงูุณูุงู 1500 ุญุฑู** ูุงูู ููุนุธู ุงูุฃุณุฆูุฉ
   - ุฅุฐุง ุงุญุชุฌุช ุณูุงู ุฃุทูู ูุณุคุงู ูุนููุ ูููู ุฒูุงุฏุชู ุฅูู 1800

2. **Instruct Model** ูุฏ ูุญุชุงุฌ ุชุซุจูุช:
   ```bash
   ollama pull llama3:8b-instruct
   ```

3. **Temperature = 0.0** ูุถูู ุฏูุฉ ุนุงููุฉ
   - ูุฏ ูุฌุนู ุงูุฅุฌุงุจุงุช ุฃูุซุฑ "ุฌูุงูุงู" ููููุง ุฃูุซุฑ ุฏูุฉ

## ๐ ุงูุชุนุฏููุงุช ุงููุณุชูุจููุฉ

- [ ] ูุฑุงูุจุฉ ุฃุฏุงุก ุงููุธุงู ุจุนุฏ ุงูุชุญุฏูุซ
- [ ] ุถุจุท ุทูู ุงูุณูุงู ุญุณุจ ููุน ุงูุณุคุงู
- [ ] ุฅุถุงูุฉ caching ููุฃุณุฆูุฉ ุงููุชูุฑุฑุฉ

---

**ุชู ุงูุชุญุฏูุซ:** 2024-12-07
**ุงูุฅุตุฏุงุฑ:** 2.6.0

